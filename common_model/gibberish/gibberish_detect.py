import os
import sys
import re
import time
import logging
import pandas as pd


# å¼•ç”¨åº“
from langdetect import detect

# å½“æ–‡æœ¬è¿‡çŸ­æˆ–æ¨¡ç³Šæ—¶ï¼Œåˆ¤æ–­å‡ºæ¥çš„ç»“æœä¼šä¸ç¡®å®šï¼›
# å¦‚æœè¦è®©ç»“æœå”¯ä¸€ï¼Œæ·»åŠ ä»¥ä¸‹ä¸¤è¡Œï¼š
from langdetect import DetectorFactory
from common_model.gibberish.zh_character_trans import ZhCharacterTrans  # NOQA: E402
from common_model.gibberish.markov_train import MarkovReport  # NOQA: E402


DetectorFactory.seed = 0


class GibberishDetector:
    def __init__(self, vocab_file=None, model_file=None, logger=None) -> None:
        # æ—¥å¿—
        if logger is None:
            self.logger = logging
        else:
            self.logger = logger
        # ç¹ä½“å­—è½¬ç®€ä½“å­—
        self.zh_trans = ZhCharacterTrans()
        # é©¬å°”ç§‘å¤«æ¨¡å‹
        self.markov_report = MarkovReport(
            vocab_file=vocab_file, logger=logger, model_file=model_file
        )

    # è¿‡æ»¤æ•°å­—+å­—æ¯
    def filter_num_letters(self, line):
        pattern = re.compile(r"[a-zA-Z0-9_]")
        line_sub = re.sub(pattern, "", line)
        return line_sub

    # è¿‡æ»¤httpé“¾æ¥
    def filter_http(self, line):
        pattern = re.compile(
            r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
        )
        line_sub = re.sub(pattern, "", line)
        return line_sub

    def is_empty(self, line):
        if line.strip() == "":
            return True
        return False

    def is_special_square(self, line):
        pattern_list = ["ï¿½", "â—†"]
        pattern = re.compile("|".join(pattern_list))
        rs = re.findall(pattern, line)
        if len(rs) > 0:
            return True
        return False

    # égb2312å­—ç¬¦é›†å­—ç¬¦å æ¯”
    def is_unrecognize_character_unusual(self, line):
        unrecognize_cnt = 0
        for w in line:
            try:
                w.encode("gb2312")
            except UnicodeEncodeError:
                unrecognize_cnt += 1
        if len(line) <= 15:
            return False
        pct = -1 if len(line) == 0 else unrecognize_cnt / len(line)
        if pct > 0.2:
            return True
        return False

    def is_ar(self, line):
        try:
            if detect(line) == "ar":
                return True
        except:
            return False
        return False

    def predict(self, line):
        rs = {"predict": 0, "reason": "", "prob": 0}

        # ç©ºå­—ç¬¦ä¸²
        empty = self.is_empty(line)
        if empty:
            rs["reason"] = "empty"
            rs["type"] = "others"
            rs["predict"] = 0
            return rs

        # è¿‡æ»¤httpé“¾æ¥
        start_time = time.time()
        try:
            line_sub = self.filter_http(line)
            # è¿‡æ»¤åä½ç©ºä¸²
            if self.is_empty(line_sub):
                rs["reason"] = "http"
                rs["type"] = "others"
                rs["predict"] = 1
                return rs
        except:
            line_sub = line
        self.logger.info("filter http cost:%s" % (time.time() - start_time))

        # çº¯æ•°å­—
        if line_sub.isdigit():
            if len(line_sub) == 11:
                rs["reason"] = "11 numbers"
                rs["type"] = "others"
                rs["predict"] = 0
            rs["reason"] = "pure number"
            rs["predict"] = 1
            return rs

        # è¿‡æ»¤æ•°å­—+å­—æ¯é“¾æ¥
        start_time = time.time()
        try:
            line_sub = self.filter_num_letters(line_sub)
            # è¿‡æ»¤åä½ç©ºä¸²
            if self.is_empty(line_sub):
                rs["reason"] = "num letters"
                rs["type"] = "others"
                rs["predict"] = 1
                return rs
        except:
            pass
        self.logger.info("filter num letters cost:%s" % (time.time() - start_time))

        # ä¹±ç square
        start_time = time.time()
        try:
            if self.is_special_square(line_sub):
                rs["reason"] = "square"
                rs["type"] = "messy code"
                rs["predict"] = 1
                return rs
        except:
            pass
        self.logger.info("filter num letters cost:%s" % (time.time() - start_time))

        # ç¹ä½“è½¬ç®€ä½“
        start_time = time.time()
        line_sc = self.zh_trans.tc_2_sc(line_sub)
        self.logger.info("tc_2_sc cost:%s" % (time.time() - start_time))

        # ä¸å¯è¯†åˆ«å­—ç¬¦å æ¯”å¼‚å¸¸
        start_time = time.time()
        unrecognize_character_unusual = self.is_unrecognize_character_unusual(line_sc)
        self.logger.info(
            "is_unrecognize_character_unusual cost:%s" % (time.time() - start_time)
        )
        if unrecognize_character_unusual:

            # ç»´å¾å°”è¯­æ£€æµ‹
            start_time = time.time()
            ar = self.is_ar(line_sc)
            self.logger.info("is_ar cost:%s" % (time.time() - start_time))
            if ar:
                rs["reason"] = "ar"
                rs["type"] = "ar"
                rs["predict"] = 0
                return rs

            rs["reason"] = "unrecognize_character"
            rs["type"] = "messy code"
            rs["predict"] = 1
            return rs

        if len(line_sc) <= 20:
            rs["reason"] = "too short"
            rs["type"] = "messy code"
            rs["predict"] = 0
            return rs

        predict_rs = self.markov_report.predict(line, self.markov_report.thresh)
        if predict_rs["predict"] == 1:
            predict_rs["type"] = "messy code"
        else:
            predict_rs["type"] = "others"
        return predict_rs


if __name__ == "__main__":
    if sys.platform == "darwin":
        DATA_DIR = "/Users/xiangyuwang/Desktop/report"
        MODEL_DIR = "/Users/xiangyuwang/Desktop/report"
    elif sys.platform == "linux":
        DATA_DIR = "/dockerdata/txwsreport_automation/data/"
        MODEL_DIR = "/dockerdata/txwsreport_automation/gibberish"
    vocab_file = os.path.join(MODEL_DIR, "vocab.txt")
    model_file = os.path.join(DATA_DIR, "markov_report.pkl")
    test_file = os.path.join(MODEL_DIR, "test_20230129_20230204.txt")
    predict_file = os.path.join(MODEL_DIR, "predict_20230129_20230204.csv")

    from common.log_factory import logger

    line = "=â—†â—†â—†)â—†å·²æŠ¥è­¦å¤„ç†â—†MqIâ—†tâ—†â—†èµ°ç§ç‹©çŒä¿æŠ¤åŠ¨ç‰©â—†â—†â—†â—†â—†â—†â—† Eoâ—†eâ—†ç›—å–ä»–äººqqå·â—†â—†.l8â—†â—†xâ—†(:}â—†â—†Våˆ©ç”¨qqæ•£æ’­çŠ¯ç½ªä¿¡æ¯uowuâ—†â—†sâ—†â—†è¾±éª‚è…¾è®¯å®¢æœVâ—†â—†â—†â—†Xæ°¸ä¹…å°ç¦â—†vL>ç™½å·â—†câ—†â—†+â—†â—†â—†EVè´©å–VPNâ—†4BVâ—†â—†fTâ—†Ryâ—†;â—†â—†â—†Nè¾±éª‚è…¾è®¯å®¢æœâ—†C<â—†â—†â—†oæé€Ÿå‡ºç»“æœâ—†â—†;aâ—†S â—†kâ—†â—†Nâ—†â—†æé€Ÿå‡ºç»“æœâ—†oâ—†Kâ—†æ°¸ä¹…å°ç¦â—†â—†â—†è¾±éª‚ä¹ è¿‘å¹³â—†æ¢§æ¡wwgæ‹·æ‰“ç™½å·â—†"
    gibberish_detector = GibberishDetector(
        vocab_file=vocab_file, logger=logger, model_file=model_file
    )
    predict_rs = gibberish_detector.predict(line)
    print(predict_rs)
    # probs = []
    # counter = 0
    # for data in open(test_file).readlines():
    #     counter += 1
    #     if counter == 1:
    #         continue
    #     field = data.strip().split("\001")
    #     line = field[2]
    #     predict_rs = gibberish_detector.predict(line)
    #     if counter % 1000 == 0:
    #         print("counter:%s" % counter)
    #     probs.append(
    #         [
    #             field[0],
    #             int(field[1]),
    #             predict_rs["prob"],
    #             predict_rs["reason"],
    #             predict_rs["predict"],
    #             field[2],
    #         ]
    #     )

    # test_df = pd.DataFrame(
    #     probs,
    #     columns=["task_id", "label", "prob", "reason", "predict", "line"],
    # )
    # test_df.to_csv(predict_file, index=False, sep="\001")
